{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assumed-singing",
   "metadata": {},
   "source": [
    "## Task 1: Get Info Box, store it to python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-packaging",
   "metadata": {},
   "source": [
    "###### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broadband-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-behavior",
   "metadata": {},
   "source": [
    "##### Load the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "touched-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://en.wikipedia.org/wiki/Toy_Story_3\")\n",
    "\n",
    "# Convert to beautiful soup object\n",
    "soup = bs(r.content)\n",
    "\n",
    "# Print out HTML\n",
    "contents = soup.prettify()\n",
    "# print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dimensional-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_box = soup.find(class_=\"infobox vevent\")\n",
    "# print(info_box.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "white-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_rows = info_box.find_all(\"tr\")\n",
    "\n",
    "# for row in info_rows:\n",
    "#     print(row.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "universal-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_value(row_data):\n",
    "    if row_data.find(\"li\"):\n",
    "        return [li.get_text(\" \", strip=True).replace(\"\\xa0\", \" \") for li in row_data.find_all(\"li\")]\n",
    "    else:\n",
    "        return row_data.get_text(\" \", strip=True).replace(\"\\xa0\", \" \")\n",
    "\n",
    "movie_info = {}\n",
    "for index, row in enumerate(info_rows):\n",
    "    if index == 0:\n",
    "        movie_info['title'] = row.find(\"th\").get_text(\" \", strip=True)\n",
    "    elif index == 1:\n",
    "        continue\n",
    "    else:\n",
    "        content_key = row.find(\"th\").get_text(\" \", strip=True)\n",
    "        content_value = get_content_value(row.find(\"td\"))\n",
    "        movie_info[content_key] = content_value\n",
    "        \n",
    "# movie_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-jerusalem",
   "metadata": {},
   "source": [
    "## Task 2: Get info box for all movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cultural-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://en.wikipedia.org/wiki/List_of_Walt_Disney_Pictures_films')\n",
    "\n",
    "# Convert to a beautiful soup object\n",
    "soup = bs(r.content)\n",
    "\n",
    "contents = soup.prettify()\n",
    "# contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deadly-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = soup.select('.wikitable.sortable i') # i means only italized items\n",
    "# movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "offensive-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(movies[0])\n",
    "# print(\"\")\n",
    "# print(movies[0].a['href']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "attended-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_content_value(row_data):\n",
    "#     if row_data.find(\"li\"):\n",
    "#         return [li.get_text(\" \", strip=True).replace(\"\\xa0\", \" \") for li in row_data.find_all(\"li\")]\n",
    "#     else:\n",
    "#         return row_data.get_text(\" \", strip=True).replace(\"\\xa0\", \" \")\n",
    "\n",
    "# def get_info_box(url):\n",
    "#     r = requests.get(url)\n",
    "\n",
    "#     soup = bs(r.content)\n",
    "#     info_box = soup.find(class_=\"infobox vevent\")\n",
    "#     info_rows = info_box.find_all('tr')\n",
    "    \n",
    "#     movie_info = {}\n",
    "#     for index, row in enumerate(info_rows):\n",
    "#         if index == 0:\n",
    "#             movie_info['title'] = row.find(\"th\").get_text(\" \", strip=True)\n",
    "#         elif index == 1:\n",
    "#             continue\n",
    "#         else:\n",
    "#             content_key = row.find(\"th\").get_text(\" \", strip=True)\n",
    "#             content_value = get_content_value(row.find(\"td\"))\n",
    "#             movie_info[content_key] = content_value\n",
    "#     return movie_info\n",
    "\n",
    "#TASK 3 SOLUTION\n",
    "def get_content_value(row_data):\n",
    "    if row_data.find(\"li\"):\n",
    "        return [li.get_text(\" \", strip=True).replace(\"\\xa0\", \" \") for li in row_data.find_all(\"li\")]\n",
    "    elif row_data.find(\"br\"):\n",
    "        return [text for text in row_data.stripped_strings]\n",
    "    else:\n",
    "        return row_data.get_text(\" \", strip=True).replace(\"\\xa0\", \" \")\n",
    "\n",
    "def clean_tags(soup):\n",
    "    for sup in soup.find_all(['sup','span']):\n",
    "        sup.decompose()\n",
    "        \n",
    "def get_info_box(url):\n",
    "    r = requests.get(url)\n",
    "\n",
    "    soup = bs(r.content)\n",
    "    info_box = soup.find(class_=\"infobox vevent\")\n",
    "    info_rows = info_box.find_all('tr')\n",
    "    \n",
    "    clean_tags(soup)\n",
    "    \n",
    "    movie_info = {}\n",
    "    for index, row in enumerate(info_rows):\n",
    "        if index == 0:\n",
    "            movie_info['title'] = row.find(\"th\").get_text(\" \", strip=True)\n",
    "        else:\n",
    "            header = row.find('th')\n",
    "            if header:\n",
    "                content_key = row.find(\"th\").get_text(\" \", strip=True)\n",
    "                content_value = get_content_value(row.find(\"td\"))\n",
    "                movie_info[content_key] = content_value\n",
    "    return movie_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "complex-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://en.wikipedia.org/wiki/List_of_Walt_Disney_Pictures_films')\n",
    "\n",
    "# Convert to a beautiful soup object\n",
    "soup = bs(r.content)\n",
    "movies = soup.select('.wikitable.sortable i a')\n",
    "# movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "about-colonial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "Zorro the Avenger\n",
      "'NoneType' object has no attribute 'find'\n",
      "The Sign of Zorro\n",
      "'NoneType' object has no attribute 'find'\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "True-Life Adventures\n",
      "'NoneType' object has no attribute 'find_all'\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "Night at the Museum: Kahmunrah Rises Again\n",
      "'NoneType' object has no attribute 'find'\n"
     ]
    }
   ],
   "source": [
    "base_path = \"https://en.wikipedia.org/\"\n",
    "movie_info_list = []\n",
    "\n",
    "for index, movie in enumerate(movies):\n",
    "    if index % 10 == 0:\n",
    "        print(index)\n",
    "    try:\n",
    "        relative_path = movie['href']\n",
    "        title = movie['title']\n",
    "        full_path = base_path + relative_path\n",
    "        \n",
    "        movie_info_list.append(get_info_box(full_path))\n",
    "#         print(relative_path)\n",
    "#         print(title)\n",
    "#         print()\n",
    "    except Exception as e:\n",
    "        print(movie.get_text())\n",
    "        print(e)\n",
    "# movie_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "infrared-sarah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Encanto',\n",
       " 'Directed by': ['Byron Howard', 'Jared Bush'],\n",
       " 'Produced by': ['Clark Spencer', 'Yvett Merino Flores'],\n",
       " 'Written by': ['Jared Bush', 'Charise Castro Smith'],\n",
       " 'Starring': 'Stephanie Beatriz',\n",
       " 'Music by': 'Lin-Manuel Miranda',\n",
       " 'Production companies': ['Walt Disney Pictures',\n",
       "  'Walt Disney Animation Studios'],\n",
       " 'Distributed by': ['Walt Disney Studios', 'Motion Pictures'],\n",
       " 'Release date': ['November 24, 2021'],\n",
       " 'Country': 'United States',\n",
       " 'Language': 'English'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_info_list[440]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "completed-submission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_info_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-brush",
   "metadata": {},
   "source": [
    "#### Save/Reload Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "detailed-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_data(title, data):\n",
    "    with open(title, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "conceptual-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data(title):\n",
    "    with open(title, encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "prospective-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_data('disney_data.json', movie_info_list)\n",
    "save_data('disney_data_cleaned.json', movie_info_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-economy",
   "metadata": {},
   "source": [
    "## Task 3: Clean our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ambient-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = load_data('disney_data_cleaned.json')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-biotechnology",
   "metadata": {},
   "source": [
    "#### Subtasks\n",
    "- ~~Clean up references [1], [2]~~\n",
    "- Convert running time into integer\n",
    "- Convert dates into datetime object\n",
    "- ~~Split up long string~~\n",
    "- Convert Budget & Box Office to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "micro-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up references [1], [2]\n",
    "## Just copy this code and paste it in the original methods (above)\n",
    "## Adding clean_tags methods\n",
    "\n",
    "# def get_content_value(row_data):\n",
    "#     if row_data.find(\"li\"):\n",
    "#         return [li.get_text(\" \", strip=True).replace(\"\\xa0\", \" \") for li in row_data.find_all(\"li\")]\n",
    "#     else:\n",
    "#         return row_data.get_text(\" \", strip=True).replace(\"\\xa0\", \" \")\n",
    "\n",
    "# def clean_tags(soup):\n",
    "#     for sup in soup.find_all(['sup','span']):\n",
    "#         sup.decompose()\n",
    "        \n",
    "# def get_info_box(url):\n",
    "#     r = requests.get(url)\n",
    "\n",
    "#     soup = bs(r.content)\n",
    "#     info_box = soup.find(class_=\"infobox vevent\")\n",
    "#     info_rows = info_box.find_all('tr')\n",
    "    \n",
    "#     clean_tags(soup)\n",
    "    \n",
    "#     movie_info = {}\n",
    "#     for index, row in enumerate(info_rows):\n",
    "#         if index == 0:\n",
    "#             movie_info['title'] = row.find(\"th\").get_text(\" \", strip=True)\n",
    "#         elif index == 1:\n",
    "#             continue\n",
    "#         else:\n",
    "#             content_key = row.find(\"th\").get_text(\" \", strip=True)\n",
    "#             content_value = get_content_value(row.find(\"td\"))\n",
    "#             movie_info[content_key] = content_value\n",
    "#     return movie_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "compliant-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the long strings\n",
    "# add elif in get_content_value\n",
    "# add else in get_info_box\n",
    "\n",
    "# def get_content_value(row_data):\n",
    "#     if row_data.find(\"li\"):\n",
    "#         return [li.get_text(\" \", strip=True).replace(\"\\xa0\", \" \") for li in row_data.find_all(\"li\")]\n",
    "#     elif row_data.find(\"br\"):\n",
    "#         return [text for text in row_data.stripped_strings]\n",
    "#     else:\n",
    "#         return row_data.get_text(\" \", strip=True).replace(\"\\xa0\", \" \")\n",
    "\n",
    "# def clean_tags(soup):\n",
    "#     for sup in soup.find_all(['sup','span']):\n",
    "#         sup.decompose()\n",
    "        \n",
    "# def get_info_box(url):\n",
    "#     r = requests.get(url)\n",
    "\n",
    "#     soup = bs(r.content)\n",
    "#     info_box = soup.find(class_=\"infobox vevent\")\n",
    "#     info_rows = info_box.find_all('tr')\n",
    "    \n",
    "#     clean_tags(soup)\n",
    "    \n",
    "#     movie_info = {}\n",
    "#     for index, row in enumerate(info_rows):\n",
    "#         if index == 0:\n",
    "#             movie_info['title'] = row.find(\"th\").get_text(\" \", strip=True)\n",
    "#         else:\n",
    "#             header = row.find('th')\n",
    "#             if header:\n",
    "#                 content_key = row.find(\"th\").get_text(\" \", strip=True)\n",
    "#                 content_value = get_content_value(row.find(\"td\"))\n",
    "#                 movie_info[content_key] = content_value\n",
    "#     return movie_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-belize",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
